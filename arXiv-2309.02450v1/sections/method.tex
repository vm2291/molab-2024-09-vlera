\section{Method}
\label{sec:method}

For our experiments, we select one model from each method identified in Section~\ref{sec:related-work:-self-supervised-video-transformers}. We take the best performing architecture and set-up described in their respective papers.

{\bf VideoMAE~\cite{Tong2022VideoMAE}.} We pre-train a standard ViT with masked reconstruction over pixels, where the masked regions are tubes across time (i.e. the same 2D mask is applied to every frame), and with an extremely high masking ratio of 90\%. As a standard ViT, this model divides frames into patches of $16 \times 16$ pixels.

{\bf SVT~\cite{Ranasinghe2021SVT}.} We extend a standard ViT previously DINO pre-trained on ImageNet with further DINO pre-training on video. That is, we take a mean teacher~\cite{tarvainen2017mean} with a global view of the data and a student with narrower spatio-temporal slices, and the objective of the student is to produce representations from these narrower views that are as close as possible to the mean teacher. As a standard ViT, its patch size is $16 \times 16$ pixels.

{\bf MaskFeat~\cite{Wei2022MaskFeat}.} We pre-train a MViTv2 model for masked reconstruction over HOG, where the masked regions are segments over time of up to half the video length, composed of multiple adjacent patches, and with a much lower masking ratio compared to VideoMAE, 40\%. As a MViT-based model, the patch size is $4 \times 4$ pixels.

{\bf BEVT~\cite{Wang2021BEVT}.} We train a VideoSwin to predict labels or ``words'' for masked regions in a video. These labels are generated by the codebook of an external discrete VAE (dVAE), separately trained on Conceptual Captions~\cite{sharma2018conceptual} as part of DALL-E~\cite{ramesh2021zero}. The masking strategy is the same as in MaskFeat, but with a slightly higher masking ratio of 50\%. VideoSwin also takes as input patches of $4 \times 4$ pixels.