\section{Conclusion}
\label{sec:conclusion}

In this work, we leverage the combined power of self-supervision and transformer-based architectures for the task of isolated sign language recognition and probe them for phonological features. Despite the small amount of data available for ISLR, self-supervised transformers produce state-of-the-art results on gloss-based WLASL2000, by leveraging continuous sign language datasets for pre-training. Probing for sign language phonological features helps us better characterize video models and pre-training tasks. For example, plain ViTs struggle at capturing fine-grained spatial relationships. Self-supervised hierarchical vision transformers behave similarly to self-supervised speech models in terms of layer-wise encoding of linguistic information.

\textbf{Future Work.} Since ISLR is a very limited task in the context of sign language understanding, a natural next step is to extend these models for other sign language tasks, most importantly sign language translation. The biggest challenge in this direction is to extend transformer-based models to more than 16 frames, which is crucial in any continuous sign language task, but requires an immense amount of compute with current attention implementations. As we have seen, probing for ASL phonological features reveals strengths and limitations of various pre-training tasks and architecture choices. Future work on self-supervised sign language models can use these features to better characterize their models, and to further expand our understanding of video models beyond action recognition and detection.
