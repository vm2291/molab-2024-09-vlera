\section{Related Work}
\label{sec:related-work}

\subsection{Isolated Sign Language Recognition}
\label{sec:related-work-islr}

The most common approach to ISLR is based on convolutional neural networks (CNNs), such as I3D~\cite{carreira2017quo}, R3D~\cite{hara2017learning}, and S3D~\cite{xie2018rethinking}. These models receive RGB videos as input, extract features from them, and use these features to assign a label for the corresponding sign~\cite{Albanie2020BSL1K, Varol2021BSL1KImp, zuo2023natural}. Other approaches employ pose-based models to mitigate variations naturally present in video that are not related to the task of identifying signs~\cite{jiang2021skeleton, jiang2021sign, Dafnis2022Bidirectional}.

Choosing the right pre-training tasks and set-ups has largely been the determining factor in ISLR performance, all of which involve supervision on other datasets. The most common pre-training datasets include Kinetics400~\cite{carreira2017quo} and larger isolated sign language datasets in a language different from the target~\cite{Albanie2020BSL1K, Varol2021BSL1KImp, Novopoltsev2023Finetuning}. Recent approaches also include auxiliary tasks during ISLR training. These include weakly-supervised phonological features~\cite{kezar2023improving} and word embeddings from written language models~\cite{zuo2023natural}.

Our approach is most similar to~\cite{Novopoltsev2023Finetuning}, which uses a VideoSwin transformer with supervised pre-training on human action recognition with Kinetics400, then Russian Sign Language ISLR with a proprietary dataset, and finally fine-tuning on WLASL2000. In contrast, we pre-train on either human action videos or exclusively on ASL videos (or a combination of both) in a self-supervised manner, and then fine-tune for ISLR on WLASL2000.

\subsection{Self-Supervised Video Transformers}
\label{sec:related-work:-self-supervised-video-transformers}

In video self-supervision with transformers, there are four dominant approaches that can be broadly summarized by their choice of objective during pre-training:
masked reconstruction of pixels~\cite{Tong2022VideoMAE, feichtenhofer2022maest, Wang2023VideoMAE2}, DINO-based~\cite{caron2021emerging} representation learning using a teacher-student setup~\cite{Ranasinghe2021SVT, wang2022mvd}, masked reconstruction of hand-crafted features, most often HOG,~\cite{Wei2022MaskFeat, sun2023mme}, and BERT-like~\cite{devlin2018bert} masked token prediction using codewords generated by a discrete VAE~\cite{Wang2021BEVT, tan2021VIMPAC}. We choose one model from each category, specifically VideoMAE~\cite{Tong2022VideoMAE} for masked reconstruction over pixels, SVT~\cite{Ranasinghe2021SVT} for DINO-like representation learning, MaskFeat~\cite{Wei2022MaskFeat} for masked reconstruction over HOG, and BEVT~\cite{Wang2021BEVT} for BERT-like masked token prediction.

Additionally, there has also been a significant amount of work on custom architectures for vision transformers. ViT~\cite{dosovitskiy2020image} remains the most common architecture, which uses standard multi-head attention~\cite{vaswani2017attention}, with a fixed patch size of $16 \times 16$ pixels. Other custom architectures for vision have been proposed recently, most importantly VideoSwin~\cite{liu2022video} and MViT~\cite{fan2021multiscale, li2022mvitv2}. These use modified multi-head attention modules: window-based attention for VideoSwin and pooling attention for MViT, with much smaller input patches of $4 \times 4$ pixels. Our model choices also reflect this landscape, since VideoMAE and SVT are based on ViT, BEVT is based on VideoSwin, and MaskFeat is based on MViTv2.
